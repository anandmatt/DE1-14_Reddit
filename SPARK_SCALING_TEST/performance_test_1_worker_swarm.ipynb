{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b033705-9e3c-4cce-9033-bc846f44573e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import time\n",
    "from pyspark.sql import SparkSession, Window\n",
    "from pyspark.sql.functions import col, count, max, avg, explode, split, length, lower, struct, row_number, desc, countDistinct\n",
    "\n",
    "# Initialization\n",
    "spark_master = \"sparkmaster\"\n",
    "data_path = \"hdfs://192.168.2.157:9000/user/ubuntu/input/corpus-webis-tldr-17.json\"\n",
    "results_path = \"performance_test_1_worker_swarm.csv\"\n",
    "\n",
    "# Functions\n",
    "def create_spark_session(worker_count):\n",
    "    \"\"\"Initialize Spark session.\"\"\"\n",
    "    return SparkSession.builder\\\n",
    "        .master(f\"spark://{spark_master}:7077\")\\\n",
    "        .appName(f\"PerformanceTest{worker_count}Workers\")\\\n",
    "        .config(\"spark.executor.instances\", str(worker_count))\\\n",
    "        .config(\"spark.executor.cores\", \"2\")\\\n",
    "        .config(\"spark.executor.memory\", \"2g\")\\\n",
    "        .config(\"spark.driver.port\", 9999)\\\n",
    "        .config(\"spark.blockManager.port\", 10005)\\\n",
    "        .getOrCreate()\n",
    "\n",
    "def log_performance(test_id, task, duration):\n",
    "    \"\"\"Log performance results to CSV.\"\"\"\n",
    "    with open(results_path, 'a', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([test_id, task, f\"{duration:.2f}\"])\n",
    "\n",
    "def measure_time(func):\n",
    "    \"\"\"Measure execution time of a function.\"\"\"\n",
    "    start_time = time.time()\n",
    "    result = func()\n",
    "    duration = time.time() - start_time\n",
    "    return duration, result\n",
    "\n",
    "def run_tasks(spark, worker_count):\n",
    "    \"\"\"Run tasks and measure performance.\"\"\"\n",
    "    df = spark.read.json(data_path).cache()\n",
    "    df.count()  # Trigger cache\n",
    "\n",
    "    # Define tasks within this function to ensure access to the DataFrame\n",
    "    tasks = {\n",
    "        'aggregate_subreddit_stats': lambda: df.groupBy('subreddit').agg(count(\"*\").alias(\"count\"), max(\"content_len\"), avg(\"summary_len\")).count(),\n",
    "        'agg': lambda: df.groupBy('subreddit').agg(count(\"*\").alias(\"count\"), max(\"content_len\"), avg(\"summary_len\")).count(),\n",
    "        'join': lambda: df.alias(\"df1\").join(df.alias(\"df2\"), col(\"df1.id\") == col(\"df2.id\")).count(),\n",
    "        'complex_transformation': lambda: df.withColumn(\"new_content_len\", col(\"content_len\") * 2).groupBy('subreddit').agg(avg(\"new_content_len\")).collect(),\n",
    "        'window_function': lambda: df.withColumn(\"rank\", row_number().over(Window.partitionBy(\"subreddit\").orderBy(desc(\"content_len\")))).filter(col(\"rank\") <= 10).count(),\n",
    "        'explode_split': lambda: df.withColumn(\"words\", explode(split(col(\"body\"), \" \"))).groupBy(\"words\").count().orderBy(desc(\"count\")).limit(10).collect(),\n",
    "        'avg_summary_by_author': lambda: df.groupBy('author').agg(avg('summary_len').alias('avg_summary_length')).count(),\n",
    "        'distinct_subreddit_count': lambda: df.select('subreddit').distinct().count(),\n",
    "        'max_summary_length_per_subreddit': lambda: df.groupBy('subreddit').agg(max('summary_len').alias('max_summary_length')).count(),\n",
    "        'top_title_per_subreddit': lambda: df.withColumn(\"title_length\", length(col(\"title\"))).groupBy('subreddit').agg(max(struct(col('title_length'), col('title'))).alias('top_title')).count(),\n",
    "        'word_count_in_titles': lambda: df.withColumn(\"word\", explode(split(lower(col(\"title\")), \"\\\\s+\"))).groupBy('word').count().orderBy(desc('count')).limit(10).collect(),\n",
    "        # 'self_join_shuffle': lambda: df.alias(\"df1\").join(df.alias(\"df2\"), col(\"df1.subreddit\") == col(\"df2.subreddit\")).agg(count(\"*\")).collect(),\n",
    "        'cross_join': lambda: df.crossJoin(df.limit(10)).agg(count(\"*\")).collect(),  \n",
    "        'complex_aggregation': lambda: df.groupBy('subreddit').agg(count(\"*\"), avg(\"summary_len\"), max(\"content_len\"), countDistinct(\"author\")).collect(),\n",
    "    }\n",
    "\n",
    "    for task_name, task_func in tasks.items():\n",
    "        duration, _ = measure_time(task_func)\n",
    "        print(f\"Task {task_name} with {worker_count} workers completed in {duration:.2f} seconds.\")\n",
    "        log_performance(f\"{worker_count}_workers\", task_name, duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef3fab06-296b-49ad-9cf1-57e004324949",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/03/15 13:41:57 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/03/15 13:42:18 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task aggregate_subreddit_stats with 1 workers completed in 658.27 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task agg with 1 workers completed in 964.41 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task join with 1 workers completed in 2091.66 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task complex_transformation with 1 workers completed in 763.23 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task window_function with 1 workers completed in 1112.51 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task explode_split with 1 workers completed in 671.76 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task avg_summary_by_author with 1 workers completed in 609.29 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task distinct_subreddit_count with 1 workers completed in 555.86 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task max_summary_length_per_subreddit with 1 workers completed in 625.32 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task top_title_per_subreddit with 1 workers completed in 630.10 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task word_count_in_titles with 1 workers completed in 331.24 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task cross_join with 1 workers completed in 431.44 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task complex_aggregation with 1 workers completed in 315.27 seconds.\n"
     ]
    }
   ],
   "source": [
    "# Main\n",
    "if __name__ == \"__main__\":\n",
    "    # Setup CSV for results\n",
    "    with open(results_path, 'w', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow(['TestID', 'Task', 'Duration'])\n",
    "    \n",
    "# Run tasks for 1 worker\n",
    "worker_counts = 1\n",
    "\n",
    "spark_session = create_spark_session(worker_counts)\n",
    "run_tasks(spark_session, worker_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26a77413-32a8-4f91-87b8-d4c8353070ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_session.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de0e2143-c938-4fd1-8b08-dbeddaeb2c66",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
