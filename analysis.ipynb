{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/03/14 13:07:20 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# Importing packages\n",
    "import numpy as np\n",
    "import pyspark\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession, SQLContext\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "spark_session = SparkSession\\\n",
    "        .builder\\\n",
    "        .master(\"spark://192.168.2.157:7077\") \\\n",
    "        .appName(\"Group14_DE\")\\\n",
    "        .config(\"spark.executor.instances\", \"4\")\\\n",
    "        .config(\"spark.executor.memory\", \"12g\")\\\n",
    "        .config(\"spark.executor.cores\", \"4\")\\\n",
    "        .config(\"spark.driver.memory\", \"2g\")\\\n",
    "        .config(\"spark.memory.fraction\", \"0.8\")\\\n",
    "        .config(\"spark.memory.storageFraction\", \"0.5\")\\\n",
    "        .config(\"spark.shuffle.service.enabled\", True)\\\n",
    "        .config(\"spark.dynamicAllocation.enabled\", True)\\\n",
    "        .config(\"spark.dynamicAllocation.minExecutors\", \"1\")\\\n",
    "        .config(\"spark.dynamicAllocation.maxExecutors\", \"4\")\\\n",
    "        .config(\"spark.dynamicAllocation.initialExecutors\", \"2\")\\\n",
    "        .config(\"spark.dynamicAllocation.executorIdleTimeout\", \"60s\")\\\n",
    "        .config(\"spark.sql.shuffle.partitions\", \"32\")\\\n",
    "        .getOrCreate()\n",
    "# Retreiving the SparkContext\n",
    "sc = spark_session.sparkContext\n",
    "\n",
    "# Adjusting the log messages \n",
    "sc.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "df = spark_session.read.json(\"hdfs://192.168.2.157:9000/user/ubuntu/input/corpus-webis-tldr-17.json\").persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- author: string (nullable = true)\n",
      " |-- body: string (nullable = true)\n",
      " |-- content: string (nullable = true)\n",
      " |-- content_len: long (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- normalizedBody: string (nullable = true)\n",
      " |-- subreddit: string (nullable = true)\n",
      " |-- subreddit_id: string (nullable = true)\n",
      " |-- summary: string (nullable = true)\n",
      " |-- summary_len: long (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the schema to understand the structure of the dataset\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/14 13:34:05 ERROR TaskSchedulerImpl: Lost executor 0 on 192.168.2.157: worker lost: Not receiving heartbeat for 60 seconds\n",
      "24/03/14 13:35:19 ERROR TaskSchedulerImpl: Lost executor 13 on 192.168.2.157: Command exited with code 137\n",
      "24/03/14 13:37:05 ERROR TaskSchedulerImpl: Lost executor 14 on 192.168.2.157: Command exited with code 137\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------+\n",
      "|          subreddit| count|\n",
      "+-------------------+------+\n",
      "|          AskReddit|589947|\n",
      "|      relationships|352049|\n",
      "|    leagueoflegends|109307|\n",
      "|               tifu| 52219|\n",
      "|relationship_advice| 50416|\n",
      "|              trees| 47286|\n",
      "|             gaming| 43851|\n",
      "|            atheism| 43268|\n",
      "|      AdviceAnimals| 40783|\n",
      "|              funny| 40171|\n",
      "+-------------------+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# get all the subreddits in a df\n",
    "subreddits_df = df.select(col(\"subreddit\")).groupBy(\"subreddit\").count().orderBy(col(\"count\").desc())\n",
    "\n",
    "subreddits_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/14 13:38:48 ERROR TaskSchedulerImpl: Lost executor 15 on 192.168.2.157: Command exited with code 137\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation between summary length and text length: 0.32852918253101326\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import length\n",
    "\n",
    "# Map each record to a DataFrame containing the length of the summary and the length of the text\n",
    "lengths_df = df.select(length(\"summary\").alias(\"summary_length\"), length(\"content\").alias(\"content_length\"))\n",
    "\n",
    "# Calculate the correlation between the two columns\n",
    "correlation = lengths_df.stat.corr(\"summary_length\", \"content_length\")\n",
    "\n",
    "print(\"Correlation between summary length and text length:\", correlation)\n",
    "\n",
    "# Stop the SparkSession\n",
    "spark_session.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
